<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="caiye">
    
    <title>
        
            深度学习复习 |
        
        業易6
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.png">
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.xml"}
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066cc","logo":"/images/logo.png","favicon":"/images/logo.png","avatar":"/images/logo.png","font_size":null,"font_family":null,"hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"header_transparent":true,"background_img":"/images/bg.svg","description":"好好学习，天天向上","font_color":null,"hitokoto":false},"scroll":{"progress_bar":false,"percent":false}},"local_search":{"enable":true,"preload":true},"code_copy":{},"code_block":{"tools":{"enable":false,"style":"default"},"highlight_theme":"default"},"side_tools":{},"pjax":{"enable":true},"lazyload":{"enable":false},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.8"},"waline":{"server_url":null,"reaction":false,"version":2}},"post":{"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"enable":false,"wordcount":false,"min2read":false},"img_align":"left","copyright_info":false},"version":"3.6.1"}
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"}
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"}
    KEEP.language_copy_copyright = {"copy":"Copy copyright info","copied":"Copied","title":"Original article title","author":"Original article author","link":"Original article link"}
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/logo.png">
                </a>
            
            <a class="logo-title" href="/">
               業易6
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                主页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                博客日志
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/links"
                            >
                                链接
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                关于
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/%E7%A7%81%E4%BA%BA%E5%8C%BA%E5%9F%9F"
                            >
                                私人区
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">主页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">博客日志</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/links">链接</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">关于</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/%E7%A7%81%E4%BA%BA%E5%8C%BA%E5%9F%9F">私人区</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            <div class="article-title">
                <span class="title-hover-animation">深度学习复习</span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/logo.png">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">caiye</span>
                            
                                <span class="author-label">Lv4</span>
                            
                        </div>
                        <div class="meta-info">
                            
<div class="article-meta-info">
    <span class="article-date article-meta-item">
        
            <i class="fa-regular fa-calendar-plus"></i>&nbsp;
        
        <span class="pc">2023-06-25 22:44:58</span>
        <span class="mobile">2023-06-25 22:44</span>
    </span>
    
        <span class="article-update-date article-meta-item">
        <i class="fas fa-file-pen"></i>&nbsp;
        <span class="pc">2023-06-26 15:29:35</span>
    </span>
    
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content keep-markdown-body">
                

                <h1 id="深度学习复习"><a href="#深度学习复习" class="headerlink" title="深度学习复习"></a>深度学习复习</h1><h2 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h2><p><strong>1.掌握随机梯度下降算法</strong></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626150802324.png" alt="image-20230626150802324"></p>
<p><strong>2.掌握批量梯度下降算法</strong></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626150814079.png" alt="image-20230626150814079"></p>
<p><strong>3.理解正则化（L1，L2）</strong></p>
<p>正则化是一种避免过拟合的技术，它通过向模型的损失函数中添加一个惩罚项来降低模型的复杂度。L1正则化和L2正则化是最常见的两种正则化方法。</p>
<p><strong>L1正则化</strong>：也称为Lasso正则化。L1正则化将模型参数的绝对值的和作为惩罚项加到损失函数中。其形式是：</p>
<pre><code>L1: λ*Σ|w_i|
</code></pre>
<p>其中λ是正则化系数，w_i是模型参数，Σ表示对所有模型参数求和。L1正则化会倾向于产生少量的特征，而其他的特征都是零，这是因为L1正则化会将不重要的特征的权重压缩为零。这种特性使得L1正则化可以被用作特征选择。</p>
<p><strong>L2正则化</strong>：也称为岭回归或权重衰减。L2正则化将模型参数的平方和作为惩罚项加到损失函数中。其形式是：</p>
<pre><code>L2: λ*Σ(w_i)^2
</code></pre>
<p>其中λ是正则化系数，w_i是模型参数，Σ表示对所有模型参数求和。相较于L1正则化，L2正则化会保留更多的特征（即，所有特征的权重都会非零），但权重会接近于零。这意味着L2正则化可以防止某些特征对预测结果的影响过大。</p>
<p>在这两个公式中，λ是一个超参数，决定了正则化的强度。如果λ为零，则正则化项没有效果。如果λ非常大，那么正则化项的影响会增大，模型的权重将被强制为零或非常接近零。</p>
<p>总的来说，L1和L2正则化都是一种在模型优化过程中引入的对模型复杂度的约束，目的是防止模型过于复杂而导致过拟合，以提高模型的泛化能力。</p>
<p><strong>4.理解Droupout思想，处理流程</strong></p>
<p>Dropout是一种用于防止神经网络过拟合的正则化技术，它在训练过程中随机关闭（即，将其输出设置为0）神经网络中的一部分神经元。Dropout通过引入网络内部的噪声，从而使模型变得更加健壮，并提高了泛化能力。</p>
<p>以下是Dropout的基本处理流程：</p>
<ol>
<li><p><strong>初始化</strong>：选择一个Dropout比率，这是在训练过程中将要随机关闭的神经元的比例。例如，Dropout比率为0.5意味着在每次训练过程中，每一层的一半神经元都将被随机关闭。</p>
</li>
<li><p><strong>训练过程</strong>：在每次训练迭代中，首先选择要关闭的神经元。关闭的神经元在前向传播和反向传播过程中都不会有任何贡献。即，在前向传播过程中，被关闭的神经元的输出将被设置为0；在反向传播过程中，不会将任何梯度传播到被关闭的神经元。然后，用剩余的”活跃”神经元进行前向传播、计算损失、反向传播和参数更新。</p>
</li>
<li><p><strong>测试过程</strong>：在测试过程中（即，使用模型进行预测时），通常不使用Dropout，所有的神经元都会被使用。为了补偿训练过程中神经元的关闭，需要将每个神经元的输出乘以Dropout比率。这可以确保测试过程中神经元的期望输出与训练过程一致。</p>
</li>
</ol>
<p>Dropout的主要优点是它可以有效地防止神经网络的过拟合，而且实现简单，计算成本低。同时，因为在每次训练迭代中都会随机关闭一部分神经元，这使得网络的不同部分可以更独立地学习特征，这通常可以提高模型的泛化能力。</p>
<p><strong>5.掌握三大定理：没有免费午餐定理，丑小鸭定理，奥卡姆剃刀原理等</strong></p>
<p><strong>没有免费午餐定理（No Free Lunch Theorem）</strong></p>
<p>在机器学习领域，没有免费午餐（No Free Lunch, NFL）定理指的是不存在一种“最佳”或“通用”的机器学习算法，适用于所有的问题。换句话说，对于特定的问题，某一种机器学习算法可能会胜过其他算法，但对于另一种问题，情况可能就完全相反了。因此，选择最适合的算法通常取决于问题本身和相关数据。</p>
<p><strong>丑小鸭定理（Ugly Duckling Theorem）</strong></p>
<p>丑小鸭定理是由美国计算机科学家Watanabe提出的，它的主要观点是：在没有先验知识的情况下，所有事物之间的相似性都是一样的。在机器学习和模式识别领域，这个定理意味着我们不能在没有先验知识的情况下，确定哪种特征更加重要或者哪种度量方式更好。</p>
<p><strong>奥卡姆剃刀原理（Occam’s Razor）</strong></p>
<p>奥卡姆剃刀是一个科学和哲学中的原理，它主张在所有可能的解释或模型中，最简单的那一个通常是正确的。在机器学习中，这个原理常常被理解为：如果两个模型对数据有同样好的拟合效果，那么我们应该选择更简单的模型，因为简单的模型通常有更好的泛化能力，不容易产生过拟合。</p>
<p>以上三个定理都是在理论上引导我们如何更好地理解和选择机器学习模型，为我们提供了不同的视角来看待机器学习问题。</p>
<h2 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h2><h3 id="理解经验风险和结构风险"><a href="#理解经验风险和结构风险" class="headerlink" title="理解经验风险和结构风险"></a>理解经验风险和结构风险</h3><p>经验风险和结构风险都是机器学习中的重要概念，它们都与模型的性能和过拟合风险有关。</p>
<ol>
<li><p><strong>经验风险</strong>：经验风险是模型在训练数据上的平均损失。它度量了模型对训练数据的拟合程度。经验风险最小化是机器学习中常用的一种方法，其目标是在训练集上找到使得平均损失最小的模型。然而，完全依赖于经验风险最小化可能导致过拟合，因为模型可能过于复杂，过度地拟合了训练数据中的噪声和异常值。</p>
</li>
<li><p><strong>结构风险</strong>：结构风险则是对经验风险的一个修正，以防止过拟合。结构风险最小化考虑了模型的复杂度，通过在经验风险的基础上添加一个正则项（对应模型复杂度）来避免过拟合。正则项的存在使得模型需要在拟合数据（经验风险小）与保持简单（模型复杂度小）之间找到一个平衡。在许多情况下，结构风险最小化能够得到更好的泛化性能。</p>
</li>
</ol>
<p>简单地说，经验风险关注模型在训练数据上的表现，而结构风险则同时考虑了模型在训练数据上的表现和模型的复杂度，以期获得更好的泛化能力。</p>
<h2 id="第四章-前馈神经网络"><a href="#第四章-前馈神经网络" class="headerlink" title="第四章 前馈神经网络"></a>第四章 前馈神经网络</h2><h3 id="什么是relu"><a href="#什么是relu" class="headerlink" title="什么是relu"></a>什么是relu</h3><p>ReLU函数，全称为线性整流函数（Rectified Linear Unit），是神经网络中常用的激活函数之一。ReLU函数的定义很简单，对于输入x，如果x大于0，ReLU函数返回x；如果x小于或等于0，ReLU函数返回0。</p>
<p>用数学公式表示，ReLU函数可以定义为：</p>
<pre><code>f(x) = max(0, x)
</code></pre>
<p>ReLU函数的图像是一个在0点断开的线性函数，对于所有正的输入，函数的值等于输入本身；对于所有负的输入，函数的值为0。</p>
<p>ReLU函数在神经网络中的应用具有一些优点：</p>
<ul>
<li><strong>简单</strong>：ReLU函数非常简单，易于计算，而且在反向传播时，它的导数要么是0（当x≤0时），要么是1（当x&gt;0时）。</li>
<li><strong>非线性</strong>：虽然ReLU函数在正区间内是线性的，但它是一个非线性函数，因此可以处理复杂的数据关系。</li>
<li><strong>避免梯度消失</strong>：对于深度神经网络，使用Sigmoid函数或者tanh函数等会产生梯度消失问题，而ReLU函数在正区间内的梯度为常数，因此可以缓解梯度消失问题。</li>
</ul>
<p>虽然ReLU函数有上述优点，但它也有一个缺点，就是当输入为负数时，ReLU函数的梯度为0，这可能会导致一些神经元”死亡”，即这些神经元将无法再对任何数据进行学习。为了解决这个问题，人们提出了一些ReLU函数的变体，如Leaky ReLU、Parametric ReLU（PReLU）和Exponential Linear Unit（ELU）等。</p>
<h3 id="激活函数优缺点"><a href="#激活函数优缺点" class="headerlink" title="激活函数优缺点"></a>激活函数优缺点</h3><p>好的，我会分析几种常见的激活函数（ReLU, Sigmoid, Tanh 和 Leaky ReLU）并给出对应的公式。</p>
<ol>
<li><p><strong>ReLU (Rectified Linear Unit)</strong></p>
<p>公式：f(x) &#x3D; max(0, x)</p>
<p>优点：</p>
<ul>
<li>计算效率高，因为ReLU只需要判断输入是否大于0。</li>
<li>不会出现梯度饱和问题，这在神经网络的反向传播中很有用，能解决深层网络的梯度消失问题。</li>
</ul>
<p>缺点：</p>
<ul>
<li>ReLU函数在x&lt;0时，梯度为0，所以一旦神经元进入这种状态就无法再次激活，这就是所谓的”死亡ReLU”问题。</li>
</ul>
</li>
<li><p><strong>Sigmoid</strong></p>
<p>公式：f(x) &#x3D; 1 &#x2F; (1 + exp(-x))</p>
<p>优点：</p>
<ul>
<li>Sigmoid函数输出的是0到1之间的数，有明确的“开”、“关”的特性（接近0就是关，接近1就是开）。</li>
</ul>
<p>缺点：</p>
<ul>
<li>当输入稍微远离了原点，Sigmoid函数就会饱和，导致梯度接近0，这在神经网络的反向传播中可能导致权重更新变慢或者不更新，这是著名的梯度消失问题。</li>
<li>Sigmoid函数的输出并非以0为中心的，这可能会导致优化过程中的收敛变慢。</li>
</ul>
</li>
<li><p><strong>Tanh</strong></p>
<p>公式：f(x) &#x3D; (exp(x) - exp(-x)) &#x2F; (exp(x) + exp(-x))</p>
<p>优点：</p>
<ul>
<li>Tanh函数输出的是-1到1之间的数，其输出以0为中心，这是tanh相比于sigmoid的一个优势。</li>
</ul>
<p>缺点：</p>
<ul>
<li>同Sigmoid一样，Tanh函数也有梯度消失和梯度饱和的问题。</li>
</ul>
</li>
<li><p><strong>Leaky ReLU</strong></p>
<p>公式：f(x) &#x3D; max(0.01x, x)</p>
<p>优点：</p>
<ul>
<li>Leaky ReLU试图解决ReLU的”死亡”问题，它允许在x&lt;0时有一个非零的梯度，使得死亡神经元有可能重新激活。</li>
</ul>
<p>缺点：</p>
<ul>
<li>Leaky ReLU的梯度在x&lt;0时依然很小，可能还是不利于神经网络的学习。</li>
</ul>
</li>
</ol>
<p>这些都是非常常用的激活函数，你可以根据具体的任务和需求来选择使用哪种激活函数。</p>
<h2 id="第五章-卷积"><a href="#第五章-卷积" class="headerlink" title="第五章 卷积"></a>第五章 卷积</h2><p><strong>卷积神经网络的特点（对比全连接网络）</strong></p>
<p>卷积神经网络（Convolutional Neural Networks, CNN）和全连接网络（Fully Connected Networks, FCN）都是神经网络的一种，不过他们在结构和应用方面有一些明显的不同。</p>
<p><strong>卷积神经网络（CNN）</strong>：</p>
<ol>
<li><p>局部连接：在CNN中，每个神经元仅与输入数据的一个局部区域相连，这与视觉系统的结构相似，对于图像这类数据具有很高的效率和有效性。</p>
</li>
<li><p>权重共享：CNN中的卷积层通过权重共享的方式，减少了需要学习的参数数量，有效降低了过拟合的风险。</p>
</li>
<li><p>空间或者时间上的次采样</p>
</li>
</ol>
<p>主要应用：图像识别、视频处理、自然语言处理等领域。</p>
<p><strong>全连接神经网络（FCN）</strong>：</p>
<ol>
<li><p>全连接：在FCN中，每一个神经元都与前一层的所有神经元连接，这使得网络可以学习输入数据的全局模式，但相比之下参数量较大，容易出现过拟合。</p>
</li>
<li><p>无平移不变性：如果输入图像或数据发生微小的位移或旋转，可能就会影响FCN的性能，因为它们对输入数据的位置非常敏感。</p>
</li>
<li><p>参数量大：由于每个神经元都与前一层的所有神经元相连，所以全连接网络通常有非常大的参数量，这使得它们需要更多的数据来防止过拟合，并且需要更多的计算资源。</p>
</li>
</ol>
<p>主要应用：全连接层通常在神经网络的末端使用，用于根据之前层的输出进行分类或回归等任务。</p>
<p>总结来说，卷积神经网络和全连接网络各有优势，选择哪种类型的网络取决于你的具体任务和数据。对于图像和视频这样的数据，CNN由于其结构特性通常能获得更好的效果。而在一些全局信息更重要的任务中，全连接网络可能会更加有效。</p>
<h3 id="Resnet"><a href="#Resnet" class="headerlink" title="Resnet"></a>Resnet</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626121956880.png" alt="image-20230626121956880"></p>
<h3 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h3><p>卷积神经网络（CNN）的参数数量由多种因素决定，包括卷积核的大小、输入和输出的深度（也称为通道数）、全连接层的神经元数量等。</p>
<p>以下是计算各个类型层的参数数量的公式：</p>
<ol>
<li><p><strong>卷积层</strong>：参数数量由卷积核的大小、输入的深度和卷积核的数量决定。具体的计算公式为：(卷积核的宽度 * 卷积核的高度 * 输入深度 + 1偏置) * 卷积核数量。如果输入深度为D，卷积核的尺寸为KxK（宽度为K，高度为K），卷积核的数量为N，那么卷积层的总参数数量为：(K * K * D + 1) * N。</p>
</li>
<li><p><strong>全连接层</strong>：全连接层的参数数量由该层的神经元数量以及前一层的输出大小决定。如果全连接层的神经元数量为N，前一层的输出大小为M，那么全连接层的参数数量为N * (M + 1)，这里+1是因为每个神经元都有一个偏置参数。</p>
</li>
</ol>
<p>卷积层的输出大小由输入大小、卷积核的大小、步长（stride）和填充（padding）决定。其计算公式为：（输入大小 - 卷积核大小 + 2 * padding）&#x2F; stride + 1。例如，如果输入的宽度和高度分别为W和H，卷积核的大小为K，步长为S，填充为P，卷积核的数量为N，那么输出的宽度和高度都是（W - K + 2 * P）&#x2F; S + 1，输出的深度则为N。</p>
<p>举例来说，假设输入的尺寸为32x32x3，卷积层使用了64个5x5的卷积核，步长为1，没有填充（padding&#x3D;0），则卷积层的参数数量为（5 * 5 * 3 + 1）* 64 &#x3D; 4864，输出的尺寸为（32 - 5 + 2 * 0）&#x2F; 1 + 1 &#x3D; 28，所以输出的尺寸为28x28x64。</p>
<p>希望这些信息对你有所帮助！</p>
<h2 id="第六章-循环神经网络"><a href="#第六章-循环神经网络" class="headerlink" title="第六章 循环神经网络"></a>第六章 循环神经网络</h2><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626120451371.png" alt="image-20230626120451371"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626121732344.png" alt="image-20230626121732344"></p>
<p>循环神经网络（Recurrent Neural Networks，RNN）和长短期记忆网络（Long Short Term Memory，LSTM）都是处理序列数据的重要工具。然而，LSTM在设计上有些重要的改进，使得它可以解决RNN在处理长序列时遇到的一些问题。这些优点和缺点包括：</p>
<p><strong>LSTM的优点：</strong></p>
<ol>
<li><p><strong>解决梯度消失问题：</strong>RNN在处理长序列时会遇到梯度消失问题，即当序列过长时，模型很难学习到序列早期的信息。LSTM通过引入“门”机制和单元状态（Cell State）来保留长期的信息，有效地解决了这个问题。</p>
</li>
<li><p><strong>长期依赖：</strong>由于其设计，LSTM能够处理具有长期依赖的序列，比如在文本生成、语音识别或机器翻译等任务中，需要理解并记忆长期上下文信息。</p>
</li>
</ol>
<p><strong>LSTM的缺点：</strong></p>
<ol>
<li><p><strong>复杂性：</strong>LSTM比基础的RNN更复杂，包含的参数更多，需要更长的训练时间，并且需要更多的计算资源。</p>
</li>
<li><p><strong>过拟合：</strong>由于LSTM的模型复杂度较高，如果训练数据量不足或模型训练不当，可能会导致过拟合。</p>
</li>
<li><p><strong>难以解释：</strong>LSTM的内部机制比基础的RNN复杂，这使得它的内部工作方式更难以解释和理解。</p>
</li>
</ol>
<p>相比之下，RNN的优点是其模型较为简单，计算量较小，而且模型的理解和解释相对较为直观。然而，其缺点主要是难以处理长序列和长期依赖问题，即梯度消失问题。</p>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626121814840.png" alt="image-20230626121814840"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626121802861.png" alt="image-20230626121802861"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626121835315.png" alt="image-20230626121835315"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626121901736.png" alt="image-20230626121901736"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626121923969.png" alt="image-20230626121923969"></p>
<p>长短期记忆网络（Long Short Term Memory，LSTM）和门控循环单元（Gated Recurrent Unit，GRU）都是为了解决循环神经网络（Recurrent Neural Networks，RNN）在处理长序列时遇到的梯度消失和梯度爆炸问题而设计的。</p>
<ol>
<li><p><strong>参数数量：</strong> GRU相对于LSTM的一个主要优势是它的模型参数更少。一个GRU单元只有两个门（重置门和更新门），而LSTM有三个门（遗忘门、输入门和输出门）。较少的参数意味着GRU更简单，可能需要较少的训练时间，也更不容易过拟合。</p>
</li>
<li><p><strong>复杂度：</strong> GRU比LSTM简化了很多，没有了output gate，并将forget gate和input gate合并为了一个single update gate。此外，GRU还合并了cell state和hidden state，而LSTM则将它们分开处理。</p>
</li>
<li><p><strong>性能：</strong> 尽管GRU更简单，但在实践中，LSTM和GRU的性能通常相当接近。选择使用哪一种，往往取决于特定应用的需求和限制。一些研究显示，在较短的序列中，GRU的表现可能更好一些；而在较长的序列中，LSTM的性能可能会更优。</p>
</li>
<li><p><strong>记忆能力：</strong> LSTM由于其独立的cell state和hidden state，具有长期记忆和短期记忆的能力，更能对长序列的信息进行存储和处理。而GRU由于合并了cell state和hidden state，可能在处理更长的序列时表现略逊一筹。</p>
</li>
<li><p><strong>计算效率：</strong> 由于参数更少，GRU在相同硬件上的计算效率更高，尤其在需要快速训练模型的场景中。</p>
</li>
</ol>
<p>总的来说，GRU相对于LSTM，主要的改进在于简化了模型结构，减少了参数数量，提高了计算效率，但这也可能导致了某种程度上的性能牺牲，特别是在处理长序列时。在实际使用中，哪一种模型更优会取决于具体的任务和数据。</p>
<h3 id="序列到序列模型（同步和异步）"><a href="#序列到序列模型（同步和异步）" class="headerlink" title="序列到序列模型（同步和异步）"></a>序列到序列模型（同步和异步）</h3><p>序列到序列（Seq2Seq）模型是一种使用神经网络将一个序列映射到另一个序列的方法。它主要由两部分组成：编码器（Encoder）和解码器（Decoder）。编码器将输入序列编码成一个固定的向量，解码器则将这个向量解码为输出序列。</p>
<p>在同步（Synchronous）和异步（Asynchronous）的上下文中，这通常指的是编码器和解码器的执行方式。在同步模型中，编码器和解码器同时执行；在异步模型中，编码器首先对整个输入序列进行编码，然后解码器再对该编码进行解码。</p>
<p><strong>具体应用：</strong></p>
<p>Seq2Seq模型广泛应用于自然语言处理（NLP）任务，如机器翻译、文本摘要、对话系统（chatbots）、语音识别等。例如，在机器翻译中，输入序列是源语言的句子，输出序列是目标语言的句子。</p>
<p><strong>公式表达：</strong></p>
<p>Seq2Seq模型的公式表达可以有很多变体，但其核心思想是使用编码器将输入序列x &#x3D; [x1, x2, …, xT]编码成上下文向量c，然后使用解码器将上下文向量c解码为输出序列y &#x3D; [y1, y2, …, yT’]。</p>
<p>对于基于RNN的Seq2Seq模型，公式可以表示为：</p>
<ul>
<li>Encoder: h_t &#x3D; f(x_t, h_{t-1})</li>
<li>Context Vector: c &#x3D; q({h_1, h_2, …, h_T})</li>
<li>Decoder: h’<em>t &#x3D; f(y</em>{t-1}, h’_{t-1}, c)</li>
</ul>
<p>这里的f是RNN（如LSTM或GRU）的隐藏状态更新函数，q是从所有隐藏状态生成上下文向量的函数。在最简单的Seq2Seq模型中，q通常是取最后一个隐藏状态，即c &#x3D; h_T。</p>
<p><strong>优点：</strong></p>
<ol>
<li>Seq2Seq模型可以处理变长的输入和输出序列，非常适合处理自然语言等序列数据。</li>
<li>Seq2Seq模型能够学习到输入和输出之间的复杂映射关系。</li>
<li>使用注意力机制（Attention）的Seq2Seq模型可以对输入序列的不同部分分配不同的注意力，使模型能够关注到输入序列的重要部分。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>Seq2Seq模型需要大量的训练数据和计算资源。</li>
<li>如果输入序列很长，Seq2Seq模型可能难以捕捉到序列中的长距离依赖关系。</li>
<li>在基本的Seq2Seq模型中，所有信息都需要压缩到固定大小的上下文向量中，可能会导致信息丢失。虽然注意力机制可以缓解这个问题，但并</li>
</ol>
<h2 id="第七章-网络优化"><a href="#第七章-网络优化" class="headerlink" title="第七章 网络优化"></a>第七章 网络优化</h2><h3 id="学习率调整三大算法"><a href="#学习率调整三大算法" class="headerlink" title="学习率调整三大算法"></a>学习率调整三大算法</h3><h4 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a><strong>AdaGrad算法</strong></h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626123848898.png" alt="image-20230626123848898"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626123859296.png" alt="image-20230626123859296"></p>
<h4 id="RMSprop算法"><a href="#RMSprop算法" class="headerlink" title="RMSprop算法"></a><strong>RMSprop算法</strong></h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626123925133.png" alt="image-20230626123925133"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626123935096.png" alt="image-20230626123935096"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626123945217.png" alt="image-20230626123945217"></p>
<h4 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a><strong>AdaDelta算法</strong></h4><p>AdaDelta 算法也是 AdaGrad 算法的一个改进．和 RMSprop算法类似，AdaDelta 算法通过梯度平方的指数衰减移动平均来调整学习率．此 外，AdaDelta算法还引入了每次参数更新差值Δ𝜃的平方的指数衰减权移动平均</p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626124022839.png" alt="image-20230626124022839"></p>
<h4 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626124051262.png" alt="image-20230626124051262"></p>
<h3 id="梯度优化"><a href="#梯度优化" class="headerlink" title="梯度优化"></a>梯度优化</h3><h4 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125406637.png" alt="image-20230626125406637"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125418719.png" alt="image-20230626125418719"></p>
<h4 id="Nesterov加速梯度"><a href="#Nesterov加速梯度" class="headerlink" title="Nesterov加速梯度"></a>Nesterov加速梯度</h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125431081.png" alt="image-20230626125431081"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125444693.png" alt="image-20230626125444693"></p>
<h4 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125501995.png" alt="image-20230626125501995"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125510538.png" alt="image-20230626125510538"></p>
<h3 id="梯度截断"><a href="#梯度截断" class="headerlink" title="梯度截断"></a>梯度截断</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125541612.png" alt="image-20230626125541612"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125550946.png" alt="image-20230626125550946"></p>
<h3 id="数据预处理归一化三种方法"><a href="#数据预处理归一化三种方法" class="headerlink" title="数据预处理归一化三种方法"></a>数据预处理归一化三种方法</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626130133869.png" alt="image-20230626130133869"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626130149430.png" alt="image-20230626130149430"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626130201591.png" alt="image-20230626130201591"></p>
<h3 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125748332.png" alt="image-20230626125748332"></p>
<img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125803315.png" alt="image-20230626125803315" style="zoom:150%;" />



<h3 id="逐层归一化"><a href="#逐层归一化" class="headerlink" title="逐层归一化"></a>逐层归一化</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626125956621.png" alt="image-20230626125956621"></p>
<h3 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626130011016.png" alt="image-20230626130011016"></p>
<h3 id="层归一化"><a href="#层归一化" class="headerlink" title="层归一化"></a>层归一化</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626130034815.png" alt="image-20230626130034815"></p>
<h2 id="第八章"><a href="#第八章" class="headerlink" title="第八章"></a>第八章</h2><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626135706945.png" alt="image-20230626135706945"></p>
<h3 id="自注意力模型"><a href="#自注意力模型" class="headerlink" title="自注意力模型"></a>自注意力模型</h3><p>在注意力模型尤其是 Soft Attention 模型中，Source 和输出 Target 的内容是不同的，比如中-英机器翻译，Source 对应的中文语句，Target 对应的英文语句。</p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626141459003.png" alt="image-20230626141459003"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626141515991.png" alt="image-20230626141515991"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626141533403.png" alt="image-20230626141533403"></p>
<p>自注意力模型（Self-Attention），也称为自我注意力模型或者内部注意力模型，是一种在处理序列数据时，允许每个元素都能注意到序列中其他所有元素的模型。在自然语言处理（NLP）中，它已经成为许多任务的重要组成部分，如Transformer模型和BERT模型等。</p>
<p><strong>自注意力的意义：</strong></p>
<p>自注意力的核心思想是，对于一个序列中的每个元素，都生成一个新的表示，这个新的表示是原序列中所有元素的加权求和，而这些权重由元素之间的相互关系决定。</p>
<p>自注意力允许模型并行地处理序列中的所有元素，而不像循环神经网络（RNN）那样逐个处理元素，因此计算效率更高。此外，自注意力能够捕捉到序列中任意距离的依赖关系，而不仅仅是局部或固定距离的依赖关系。</p>
<p><strong>自注意力的公式：</strong></p>
<p>假设输入序列为X &#x3D; [x_1, x_2, …, x_n]，每个x_i是一个d维的向量。自注意力首先会计算三个新的向量序列：Q（Query），K（Key）和V（Value），它们都是由X乘以不同的权重矩阵得到的：</p>
<p>Q &#x3D; XW_Q<br>K &#x3D; XW_K<br>V &#x3D; XW_V</p>
<p>这里的W_Q，W_K和W_V都是模型要学习的参数。</p>
<p>然后，计算Query和Key的点积，再除以一个缩放因子（通常是d的平方根），得到注意力得分：</p>
<p>score &#x3D; QK^T &#x2F; sqrt(d)</p>
<p>通过softmax函数，将score转化为权重：</p>
<p>weights &#x3D; softmax(score)</p>
<p>最后，用这些权重对Value序列进行加权求和，得到输出序列：</p>
<p>Z &#x3D; weights * V</p>
<p><strong>自注意力的处理流程：</strong></p>
<ol>
<li>计算Query、Key和Value序列。</li>
<li>计算注意力得分（Query和Key的点积）。</li>
<li>通过softmax函数，将注意力得分转化为权重。</li>
<li>对Value序列进行加权求和，得到输出。</li>
</ol>
<p>这种自注意力机制使得模型能够关注到序列中的任何位置，提高了模型处理长序列和捕捉长距离依赖的能力。</p>
<h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p>Transformer优点：transformer不但对seq2seq模型两点缺点有了实质性的改进(多头交互式attention模块)，而且引入了self-attention模块，让源序列和目标序列首先“自关联”起来，使得源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型，因此我认为这是transformer优于seq2seq模型的地方。</p>
<h2 id="第十四章-深度强化学习"><a href="#第十四章-深度强化学习" class="headerlink" title="第十四章  深度强化学习"></a>第十四章  深度强化学习</h2><h3 id="深度强化学习五要素"><a href="#深度强化学习五要素" class="headerlink" title="深度强化学习五要素"></a>深度强化学习五要素</h3><p>深度强化学习（Deep Reinforcement Learning, DRL）是强化学习（Reinforcement Learning, RL）和深度学习（Deep Learning, DL）的结合。在强化学习中，有五个基本组成部分，也可以看作是五个要素：</p>
<ol>
<li><p><strong>环境（Environment）</strong>：这是一个被学习的系统或问题，比如棋盘游戏、控制问题、马尔科夫决策过程等。在这个环境中，智能体（Agent）通过执行一系列的动作（Actions）来达到某种目标。</p>
</li>
<li><p><strong>智能体（Agent）</strong>：智能体是在环境中执行动作、学习和做出决策的实体。在深度强化学习中，智能体通常会有一个或多个深度神经网络来帮助它进行决策和学习。</p>
</li>
<li><p><strong>状态（State）</strong>：状态代表了环境在某一时刻的条件或信息。例如，在棋盘游戏中，状态可能包括棋盘的布局、游戏的规则等。状态为智能体提供了做出决策的基础信息。</p>
</li>
<li><p><strong>动作（Action）</strong>：动作是智能体在给定状态下可以执行的操作。例如，在棋盘游戏中，动作可能包括移动一颗棋子、跳过轮次等。</p>
</li>
<li><p><strong>奖赏（Reward）</strong>：奖赏是一个信号，表明智能体的某个动作在达到目标方面的效果如何。奖赏可以是正的（表明智能体的动作有助于达到目标），也可以是负的（表明智能体的动作阻碍了达到目标）。强化学习的目标就是通过学习找到一个策略，使得智能体能够最大化长期的累积奖赏。</p>
</li>
</ol>
<p>在深度强化学习中，上述五个要素仍然适用，只是学习和决策的过程中加入了深度学习的技术，比如使用深度神经网络来表示和学习环境的状态、智能体的策略等。</p>
<h3 id="两个值函数"><a href="#两个值函数" class="headerlink" title="两个值函数"></a>两个值函数</h3><h4 id="状态值函数"><a href="#状态值函数" class="headerlink" title="状态值函数"></a>状态值函数</h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626142614749.png" alt="image-20230626142614749"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626142705970.png" alt="image-20230626142705970"></p>
<h4 id="状态-动作值函数（-Q函数）"><a href="#状态-动作值函数（-Q函数）" class="headerlink" title="状态-动作值函数（ Q函数）"></a>状态-动作值函数（ Q函数）</h4><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626142727070.png" alt="image-20230626142727070"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626142741740.png" alt="image-20230626142741740"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626142753987.png" alt="image-20230626142753987"></p>
<h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626145756376.png" alt="image-20230626145756376"></p>
<p><img src="https://raw.githubusercontent.com/lijianye521/images/master/image-20230626145816966.png" alt="image-20230626145816966"></p>
<h3 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h3><p><img src="/../../../../../AppData/Roaming/Typora/typora-user-images/image-20230626145856591.png" alt="image-20230626145856591"></p>
<p><img src="/../../../../../AppData/Roaming/Typora/typora-user-images/image-20230626145906052.png" alt="image-20230626145906052"></p>
<h3 id="Qlearning"><a href="#Qlearning" class="headerlink" title="Qlearning"></a>Qlearning</h3><p>Q-learning是一种被广泛使用的强化学习算法。在强化学习中，智能体(agent)在环境中采取动作，以最大化其总奖励。Q-learning算法能够学习一个策略，用来告诉智能体在给定状态下应采取什么样的行动。</p>
<p>Q-learning中的”Q”代表”质量”。在Q-learning算法中，智能体学习一个函数Q，这个函数接受一个状态和一个动作作为输入，并输出一个估计值，表示在给定状态下执行该动作的预期奖励。</p>
<p>Q-learning算法的核心是基于贝尔曼方程的Q值迭代更新公式，如下所示：</p>
<p>Q(s, a) ← Q(s, a) + α [r + γ max_a’ Q(s’, a’) - Q(s, a)]</p>
<p>这里：</p>
<ul>
<li>Q(s, a)表示在状态s下执行动作a的预期奖励。</li>
<li>α 是学习率，它决定了我们应该多大程度上考虑新的信息（即奖励和下一个状态的最大Q值）。</li>
<li>r 是智能体执行动作a并转移到状态s’后收到的即时奖励。</li>
<li>γ 是折扣因子，它决定了我们应该多大程度上考虑未来的奖励。值越接近1，我们考虑的未来奖励就越多。</li>
<li>max_a’ Q(s’, a’) 是在新状态s’下可能的所有动作a’的最大预期奖励。</li>
</ul>
<p>Q-learning算法包括以下基本步骤：</p>
<ol>
<li>初始化Q值表，通常是所有零。</li>
<li>对于每一个情节(episode)，进行以下操作：<ul>
<li>选择一个状态s。</li>
<li>选择一个动作a。这通常是根据某种策略（如ε-贪婪策略）从当前状态s的可能动作中选择的。</li>
<li>执行动作a，观察奖励r和新状态s’。</li>
<li>使用上面的公式更新Q(s, a)。</li>
</ul>
</li>
</ol>
<p>这个过程会在许多情节中反复进行，直到Q值表收敛为实际值。然后，智能体可以使用这个Q值表来选择执行最优动作。</p>

            </div>

            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                               rel="prev"
                               href="/2023/08/08/JWT%E5%AD%A6%E4%B9%A0-%E7%99%BB%E5%BD%95%E9%80%BB%E8%BE%91%E5%AE%9E%E7%8E%B0/"
                            >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                                <span class="title flex-center">
                                <span class="post-nav-title-item">JWT学习--登录逻辑实现</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                               rel="next"
                               href="/2023/05/26/%E4%BA%91%E8%AE%A1%E7%AE%97%E7%BB%BC%E5%90%88%E5%AE%9E%E8%AE%AD3-%E4%B8%89%E4%B8%AA%E5%B0%8F%E5%AE%9E%E9%AA%8C/"
                            >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">云计算综合实训3-三个小实验</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                                <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                            </a>
                        </div>
                    
                </div>
            

            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0"><span class="nav-text">深度学习复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0"><span class="nav-text">第二章</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0"><span class="nav-text">第三章</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%90%86%E8%A7%A3%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E5%92%8C%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9"><span class="nav-text">理解经验风险和结构风险</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">第四章 前馈神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFrelu"><span class="nav-text">什么是relu</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-text">激活函数优缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E5%8D%B7%E7%A7%AF"><span class="nav-text">第五章 卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Resnet"><span class="nav-text">Resnet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="nav-text">参数计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">第六章 循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM"><span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU"><span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%90%8C%E6%AD%A5%E5%92%8C%E5%BC%82%E6%AD%A5%EF%BC%89"><span class="nav-text">序列到序列模型（同步和异步）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96"><span class="nav-text">第七章 网络优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E4%B8%89%E5%A4%A7%E7%AE%97%E6%B3%95"><span class="nav-text">学习率调整三大算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AdaGrad%E7%AE%97%E6%B3%95"><span class="nav-text">AdaGrad算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RMSprop%E7%AE%97%E6%B3%95"><span class="nav-text">RMSprop算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdaDelta%E7%AE%97%E6%B3%95"><span class="nav-text">AdaDelta算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-text">自适应学习率</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96"><span class="nav-text">梯度优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="nav-text">动量法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Nesterov%E5%8A%A0%E9%80%9F%E6%A2%AF%E5%BA%A6"><span class="nav-text">Nesterov加速梯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam%E7%AE%97%E6%B3%95"><span class="nav-text">Adam算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%88%AA%E6%96%AD"><span class="nav-text">梯度截断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="nav-text">数据预处理归一化三种方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="nav-text">丢弃法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%90%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">逐层归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">批量归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">层归一化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0"><span class="nav-text">第八章</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B"><span class="nav-text">自注意力模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer"><span class="nav-text">transformer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">第十四章  深度强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BA%94%E8%A6%81%E7%B4%A0"><span class="nav-text">深度强化学习五要素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E4%B8%AA%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">两个值函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">状态值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8A%B6%E6%80%81-%E5%8A%A8%E4%BD%9C%E5%80%BC%E5%87%BD%E6%95%B0%EF%BC%88-Q%E5%87%BD%E6%95%B0%EF%BC%89"><span class="nav-text">状态-动作值函数（ Q函数）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-text">策略迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-text">值迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Qlearning"><span class="nav-text">Qlearning</span></a></li></ol></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2020</span> -
            
            2023
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">caiye</a>
            
        </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.6.1</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>





    
<script src="/js/local-search.js"></script>







<div class="post-scripts pjax">
    
        
<script src="/js/post-helper.js"></script>

        
            
<script src="/js/libs/anime.min.js"></script>

        
        
            
<script src="/js/toc.js"></script>

        
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
